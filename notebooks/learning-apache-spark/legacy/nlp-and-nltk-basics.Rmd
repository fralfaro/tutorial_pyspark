---
title: "NLP and NLTK basics"
author: "Ming Chen"
date: "6/11/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

A lot of examples in this article are borrowed from the book written by **Bird et al. (2009)**. Here I tried to implement the examples from the book with spark as much as possible.

Refer to the book for more details: Bird, Steven, Ewan Klein, and Edward Loper. Natural language processing with Python: analyzing text with the natural language toolkit. " O'Reilly Media, Inc.", 2009.

## Basic terminology

* **text**: a sequence of words and punctuation.
* **frequency distribution**: the frequency of words in a text object.
* **collocation**: a **sequence of words** that occur together unusually often.
* **bigrams**: word pairs. High frequent bigrams are collocations.
* **corpus**: a large body of text
* **wordnet**: a lexical database in which english words are grouped into sets of synonyms (**also called synsets**).
* **text normalization**: the process of transforming text into a single canonical form, e.g., converting text to lowercase, removing punctuations and so on.
* **Lemmatization**: the process of grouping variant forms of the same word so that they can be analyzed as a single item.
* **Stemming**: the process of reducing inflected words to their **word stem**.
* **tokenization**:
* **segmentation**:
* **chunking**:

## Texts as lists of words

Create a data frame consisting of text elements.

```{python}
import pandas as pd
pdf = pd.DataFrame({
        'texts': [['I', 'like', 'playing', 'basketball'],
                 ['I', 'like', 'coding'],
                 ['I', 'like', 'machine', 'learning', 'very', 'much']]
    })
    
df = spark.createDataFrame(pdf)
df.show(truncate=False)
```

```
+----------------------------------------+
|texts                                   |
+----------------------------------------+
|[I, like, playing, basketball]          |
|[I, like, coding]                       |
|[I, like, machine, learning, very, much]|
+----------------------------------------+
```


## Ngrams and collocations

Transform texts to 2-grams, 3-grams and 4-grams collocations.

```{python}
from pyspark.ml.feature import NGram
from pyspark.ml import Pipeline
ngrams = [NGram(n=n, inputCol='texts', outputCol=str(n)+'-grams') for n in [2,3,4]]

# build pipeline model
pipeline = Pipeline(stages=ngrams)

# transform data
texts_ngrams = pipeline.fit(df).transform(df)
```

```{python}
# display result
texts_ngrams.select('2-grams').show(truncate=False)
texts_ngrams.select('3-grams').show(truncate=False)
texts_ngrams.select('4-grams').show(truncate=False)
```

```
+------------------------------------------------------------------+
|2-grams                                                           |
+------------------------------------------------------------------+
|[I like, like playing, playing basketball]                        |
|[I like, like coding]                                             |
|[I like, like machine, machine learning, learning very, very much]|
+------------------------------------------------------------------+

+----------------------------------------------------------------------------------+
|3-grams                                                                           |
+----------------------------------------------------------------------------------+
|[I like playing, like playing basketball]                                         |
|[I like coding]                                                                   |
|[I like machine, like machine learning, machine learning very, learning very much]|
+----------------------------------------------------------------------------------+

+---------------------------------------------------------------------------------+
|4-grams                                                                          |
+---------------------------------------------------------------------------------+
|[I like playing basketball]                                                      |
|[]                                                                               |
|[I like machine learning, like machine learning very, machine learning very much]|
+---------------------------------------------------------------------------------+
```


## Access corpora from the NLTK package

**The *gutenberg* corpus**

```{python}
## get file ids in gutenberg corpus
gutenberg_fileids = gutenberg.fileids()
gutenberg_fileids
```

```
[u'austen-emma.txt',
 u'austen-persuasion.txt',
 u'austen-sense.txt',
 u'bible-kjv.txt',
 u'blake-poems.txt',
 u'bryant-stories.txt',
 u'burgess-busterbrown.txt',
 u'carroll-alice.txt',
 u'chesterton-ball.txt',
 u'chesterton-brown.txt',
 u'chesterton-thursday.txt',
 u'edgeworth-parents.txt',
 u'melville-moby_dick.txt',
 u'milton-paradise.txt',
 u'shakespeare-caesar.txt',
 u'shakespeare-hamlet.txt',
 u'shakespeare-macbeth.txt',
 u'whitman-leaves.txt']
```

```{python}
## absolute path of a file
gutenberg.abspath(gutenberg_fileids[0])
```

```{python}
FileSystemPathPointer(u'/Users/mingchen/nltk_data/corpora/gutenberg/austen-emma.txt')
```

```{python}
## raw text
gutenberg.raw(gutenberg_fileids[0])[:200]
```

```
u'[Emma by Jane Austen 1816]\n\nVOLUME I\n\nCHAPTER I\n\n\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\nand happy disposition, seemed to unite some of the best blessings\nof existence; an'
```

```{python}
## the words of the whole corpus
gutenberg.words()
```

```
[u'[', u'Emma', u'by', u'Jane', u'Austen', u'1816', ...]
```

```{python}
len(gutenberg.words())
```

```
2621613
```

```{python}
## the sentences of a speficied file
gutenberg.sents(gutenberg_fileids[0])
```

```
[[u'[', u'Emma', u'by', u'Jane', u'Austen', u'1816', u']'], [u'VOLUME', u'I'], ...]
```

```{python}
len(gutenberg.sents(gutenberg_fileids[0]))
```

```
7752
```

**Loading custom corpus**

Let's create a corpus consisting all files from the **./data** directory.

```{python}
from nltk.corpus import PlaintextCorpusReader
corpus_data = PlaintextCorpusReader('./data', '.*')
```

Files in the corpus *corpus_data*

```{python}
data_fileids = corpus_data.fileids()
data_fileids
```

```
['Advertising.csv',
 'Credit.csv',
 'WineData.csv',
 'churn-bigml-20.csv',
 'churn-bigml-80.csv',
 'cuse_binary.csv',
 'horseshoe_crab.csv',
 'hsb2.csv',
 'hsb2_modified.csv',
 'iris.csv',
 'mtcars.csv',
 'prostate.csv',
 'twitter.txt']
```

Raw text in *twitter.txt*

```{python}
corpus_data.raw('twitter.txt')
```

```
u'Fresh install of XP on new computer. Sweet relief! fuck vista\t1018769417\t1.0\nWell. Now I know where to go when I want my knives. #ChiChevySXSW http://post.ly/RvDl\t10284216536\t1.0\n"Literally six weeks before I can take off ""SSC Chair"" off my email. Its like the torturous 4th mile before everything stops hurting."\t10298589026\t1.0\nMitsubishi i MiEV - Wikipedia, the free encyclopedia - http://goo.gl/xipe Cutest car ever!\t109017669432377344\t1.0\n\'Cheap Eats in SLP\' - http://t.co/4w8gRp7\t109642968603963392\t1.0\nTeenage Mutant Ninja Turtle art is never a bad thing... http://bit.ly/aDMHyW\t10995492579\t1.0\nNew demographic survey of online video viewers: http://bit.ly/cx8b7I via @KellyOlexa\t11713360136\t1.0\nhi all - i\'m going to be tweeting things lookstat at the @lookstat twitter account. please follow me there\t1208319583\t1.0\nHoly carp, no. That movie will seriously suffer for it. RT @MouseInfo: Anyone excited for The Little Mermaid in 3D?\t121330835726155776\t1.0\n"Did I really need to learn ""I bought a box and put in it things"" in arabic? This is the most random book ever."\t12358025545\t1.0\n'
```


Words and sentences in file *twitter.txt*.

```{python}
corpus_data.words(fileids='twitter.txt')
```

```
[u'Fresh', u'install', u'of', u'XP', u'on', u'new', ...]
```

```{python}
len(corpus_data.words(fileids='twitter.txt'))
```

```
253
```

```{python}
corpus_data.sents(fileids='twitter.txt')
```

```
[[u'Fresh', u'install', u'of', u'XP', u'on', u'new', u'computer', u'.'], [u'Sweet', u'relief', u'!'], ...]
```

```{python}
len(corpus_data.sents(fileids='twitter.txt'))
```

```
14
```


## WordNet

The `nltk.corpus.wordnet.synsets()` function load all synsents with a given lemma and part of speech tag.

Load all synsets into a spark data frame given the lemma `car`.

```{python}
pdf = pd.DataFrame({
        'car_synsets': [synsets._name for synsets in wordnet.synsets('car')]
    })
df = spark.createDataFrame(pdf)
df.show()
```

```
+--------------+
|   car_synsets|
+--------------+
|      car.n.01|
|      car.n.02|
|      car.n.03|
|      car.n.04|
|cable_car.n.01|
+--------------+
```

**Get lemma names given a synset**

```{python}
from pyspark.sql.functions import udf
from pyspark.sql.types import *
from nltk.corpus import wordnet

returntype = ArrayType(StringType())
synset_lemmas_udf = udf(lambda x: wordnet.synset(x).lemma_names(), returnType=returntype)
df_lemmas = df.select('car_synsets', synset_lemmas_udf(df.car_synsets).alias('lemma_names'))
```

```{python}
## display result
df_lemmas.show(truncate=False)
```

```
+--------------+------------------------------------------+
|car_synsets   |lemma_names                               |
+--------------+------------------------------------------+
|car.n.01      |[car, auto, automobile, machine, motorcar]|
|car.n.02      |[car, railcar, railway_car, railroad_car] |
|car.n.03      |[car, gondola]                            |
|car.n.04      |[car, elevator_car]                       |
|cable_car.n.01|[cable_car, car]                          |
+--------------+------------------------------------------+
```


**Get synset definition**

```{python}
synset_definition_udf = udf(lambda x: wordnet.synset(x).definition(), StringType())
df_2 = df_lemmas.select('car_synsets',
                        'lemma_names',
                        synset_definition_udf(df.car_synsets).alias('definition'))
```


```{python}
df_2.show()
```

```
+--------------+--------------------+--------------------+
|   car_synsets|         lemma_names|          definition|
+--------------+--------------------+--------------------+
|      car.n.01|[car, auto, autom...|a motor vehicle w...|
|      car.n.02|[car, railcar, ra...|a wheeled vehicle...|
|      car.n.03|      [car, gondola]|the compartment t...|
|      car.n.04| [car, elevator_car]|where passengers ...|
|cable_car.n.01|    [cable_car, car]|a conveyance for ...|
+--------------+--------------------+--------------------+
```

