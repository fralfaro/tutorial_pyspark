---
title: "Spark on Jupyter"
author: "Wenqiang Feng & Ming Chen"
date: "2/5/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### 1. Install jupyter with conda

```{python eval=FALSE}
conda install jupyter
```

### 2. Get `jupyter binary executable path`

```{python eval=FALSE}
which jupyter
```
```{python eval=FALSE}
# output
/Users/mingchen/anaconda2/bin/jupyter
```

### 3.Link spark with jupyter

```{python eval=FALSE}
export PYSPARK_DRIVER_PYTHON=/Users/mingchen/anaconda2/bin/jupyter
export PYSPARK_DRIVER_PYTHON_OPTS="notebook --NotebookApp.open_browser=False --NotebookApp.ip='*' --NotebookApp.port=8880"
```

You can also add the two environmental variables to the `~/.bash_profile` file to permenantly link spark with jupyter

### 4. Run jupyter notebook

```{python eval=FALSE}
pyspark
```

Then go to [http://127.0.0.1:8880](http://127.0.0.1:8880)

