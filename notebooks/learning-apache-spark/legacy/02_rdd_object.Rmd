---
title: "RDD object"
author: "Ming Chen"
date: "6/1/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

# Content
* [Create an RDD object](#create-an-rdd-object)
* [Map functions](#map-functions)
* [Aggregate functions](#aggregate-functions)
* [Summary statistics](#summary-statistics)
* [Group functions](#group-functions)
* [Merge functions](#merge functions)

## Create an RDD object

```{python}
mtcars = sc.textFile('data/mtcars.csv', use_unicode=False)
```

Display a few rows.

```{python}
mtcars.take(5)
```


```{python}
['model,mpg,cyl,disp,hp,drat,wt,qsec,vs,am,gear,carb',
 'Mazda RX4,21,6,160,110,3.9,2.62,16.46,0,1,4,4',
 'Mazda RX4 Wag,21,6,160,110,3.9,2.875,17.02,0,1,4,4',
 'Datsun 710,22.8,4,108,93,3.85,2.32,18.61,1,1,4,1',
 'Hornet 4 Drive,21.4,6,258,110,3.08,3.215,19.44,1,0,3,1']
```

## Map functions

There are several map functions that operate on RDD objects:

* `map()`
* `mapValues()`
* `flatMap()`
* `flatMapValues()`

### `map()`

The `map()` applies a function to each element of the RDD.

```{python}
# split string into array and remove the header row.
mtcars_map = mtcars.map(lambda x: x.split(',')). \
    filter(lambda x: x[0] != 'model')
    
mtcars_map.take(2)
```

```{python}
[['Mazda RX4',
  '21',
  '6',
  '160',
  '110',
  '3.9',
  '2.62',
  '16.46',
  '0',
  '1',
  '4',
  '4'],
 ['Mazda RX4 Wag',
  '21',
  '6',
  '160',
  '110',
  '3.9',
  '2.875',
  '17.02',
  '0',
  '1',
  '4',
  '4']]
```

### `mapValues()`

Each element in the RDD is a ***tuple***. The first element is the key, and the second element is the value. The `mapValues()` applies a function to the values of each elements and keep the original keys.

```{python}
# create a tuple
mtcars_tuple = mtcars_map.map(lambda x: (x[0], x[1:]))
mtcars_tuple.take(2)
```

```{python}
# note that the x below refers to the value in each tuple elements in the RDD
# it does not include the key.
mtcars_tuple.mapValues(lambda x: map(float, x) ).take(2)
```

```{python}
[('Mazda RX4',
  [21.0, 6.0, 160.0, 110.0, 3.9, 2.62, 16.46, 0.0, 1.0, 4.0, 4.0]),
 ('Mazda RX4 Wag',
  [21.0, 6.0, 160.0, 110.0, 3.9, 2.875, 17.02, 0.0, 1.0, 4.0, 4.0])]
```


### `flatMap()`

`flatMap()` applies a function to each element of the RDD and then flaten each results. In another words, each value in the returned results for each elements will become a new row.

```{python}
# this command convert the data into a data frame of two variables:
# one is car model, the other is observations from that car model.
mtcars_map.flatMap(lambda x: [(x[0], number) for number in x[1:]]).take(20)
```

```{python}
[('Mazda RX4', '21'),
 ('Mazda RX4', '6'),
 ('Mazda RX4', '160'),
 ('Mazda RX4', '110'),
 ('Mazda RX4', '3.9'),
 ('Mazda RX4', '2.62'),
 ('Mazda RX4', '16.46'),
 ('Mazda RX4', '0'),
 ('Mazda RX4', '1'),
 ('Mazda RX4', '4'),
 ('Mazda RX4', '4'),
 ('Mazda RX4 Wag', '21'),
 ('Mazda RX4 Wag', '6'),
 ('Mazda RX4 Wag', '160'),
 ('Mazda RX4 Wag', '110'),
 ('Mazda RX4 Wag', '3.9'),
 ('Mazda RX4 Wag', '2.875'),
 ('Mazda RX4 Wag', '17.02'),
 ('Mazda RX4 Wag', '0'),
 ('Mazda RX4 Wag', '1')]
```

### 'flatMapValues()'

`flatMapValues()` operates on **key-value** pair RDD and flatten the results without changing the keys.

```{python}
mtcars_tuple.flatMapValues(lambda x: x).take(20)
```

```{python}
[('Mazda RX4', '21'),
 ('Mazda RX4', '6'),
 ('Mazda RX4', '160'),
 ('Mazda RX4', '110'),
 ('Mazda RX4', '3.9'),
 ('Mazda RX4', '2.62'),
 ('Mazda RX4', '16.46'),
 ('Mazda RX4', '0'),
 ('Mazda RX4', '1'),
 ('Mazda RX4', '4'),
 ('Mazda RX4', '4'),
 ('Mazda RX4 Wag', '21'),
 ('Mazda RX4 Wag', '6'),
 ('Mazda RX4 Wag', '160'),
 ('Mazda RX4 Wag', '110'),
 ('Mazda RX4 Wag', '3.9'),
 ('Mazda RX4 Wag', '2.875'),
 ('Mazda RX4 Wag', '17.02'),
 ('Mazda RX4 Wag', '0'),
 ('Mazda RX4 Wag', '1')]
```


## Aggregate functions

Two aggregate functions:

* `aggregate()`
* `aggregateByKey()`

### `aggregate(zeroValue, seqOp, combOp)`

* `zeroValue` is like a data container. Its structure should match with the data structure
of the returned values from the ***seqOp*** function.
* `seqOp` is a function that takes **two arguments**: the first argument is the `zeroValue` and the second argument is an element from the RDD. The `zeroValue` gets updated with the returned value every run.
* `combOp` is a function that takes **two arguments**: the first argument is the final `zeroValue` from one partition and the other is another final `zeroValue` from another partition.

The code below calculates the sum of square roots for mpg and disp.

```{python}
# the data
mtcars_map.take(2)
```

```{python}
[['Mazda RX4', 21.0, 6.0, 160.0, 110.0, 3.9, 2.62, 16.46, 0.0, 1.0, 4.0, 4.0],
 ['Mazda RX4 Wag',
  21.0,
  6.0,
  160.0,
  110.0,
  3.9,
  2.875,
  17.02,
  0.0,
  1.0,
  4.0,
  4.0]]
```


```{python}
# the 2nd and 4th columns are mpg and disp, respectively.
# calculate the averages of mpg and disp
mpg_mean = mtcars_vars.map(lambda x: x[1]).mean()
disp_mean = mtcars_vars.map(lambda x: x[3]).mean()
```

Use `aggregate()` function.

```{python}
# define zeroValue
zero_value = (0, 0) # we need to calculate two variances. Our initial value has two elements
# define seqOp
seqOp = lambda z, x: ( (x[1] - mpg_mean)**2, (x[3] - disp_mean)**2 )
# define combOp
combOp = lambda px, py: ( px[0] + py[0], px[1] + py[1] )

# implements aggregate().
mtcars_vars.aggregate(zero_value, seqOp, combOp)
```

```{python}
(1126.0471875, 476184.7946875)
```

The same result was obtained by using R.

```{r}
c( (mtcars$mpg - mean(mtcars$mpg))^2 %>% sum(),
   (mtcars$disp - mean(mtcars$disp))^2 %>% sum())
```

```{python}
1126.047 476184.795
```


## `aggregateByKey()`

This function does similar things as `aggregate()`. The `aggregate()` aggregate all results to the very end, but `aggregateByKey()` merge results by key.

Again, let's do some sum of squre root calculation.

```{python}
# get some data and make it a key-value pair RDD.
iris_rdd = sc.textFile('data/iris.csv', use_unicode=False)
iris_ssr = iris_rdd.map(lambda x: x.split(',')).\
    map(lambda x: (x[-1], x[:-1])).\
    filter(lambda x: x[0] != 'species').\
    map(lambda x: (x[0], map(float, x[1])))
iris_ssr.take(5)
```

```{python}
[('setosa', [5.1, 3.5, 1.4, 0.2]),
 ('setosa', [4.9, 3.0, 1.4, 0.2]),
 ('setosa', [4.7, 3.2, 1.3, 0.2]),
 ('setosa', [4.6, 3.1, 1.5, 0.2]),
 ('setosa', [5.0, 3.6, 1.4, 0.2])]
```


Calculate averages for sepal_length and sepal_width.

```{python}
sepal_length_mean = iris_ssr.map(lambda x: x[1][1]).mean()
sepal_width_mean = iris_ssr.map(lambda x: x[1][2]).mean()
```


Use `aggregateByKey()` function.

```{python}
# define zeroValue
zero_value = (0, 0)
# define seqOp
seqOp = (lambda x, y: (x[0] + (y[1] - sepal_length_mean)**2, x[1] + (y[3] - sepal_width_mean)**2))
# define combOp
combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))
```


## Summary statistics

* `count()`: return the total number of elements in the RDD
* `max()`: return the maximum elements in the RDD
* `min()`: return minimum elements in the RDD
* `mean()`: return the mean of RDD's elements
* `sum()`: add up the elements in the RDD
* `stdev()`: return standard deviation of the RDD's elements
* `variance()`: return variance of the RDD's elements
* `stats()`: calculate statistics above.

Elements in the RDD are scalar object (single value).
```{python}
mpg.take(5)
```

```{python}
[21.0, 21.0, 22.8, 21.4, 18.7]
```

`count()`
```{python}
mpg.count()
```

```{python}
32
```

`max()`

```{python}
mpg.max()
```


```{python}
33.9
```


```{python}
mpg.min()
```

```{python}
10.4
```


`mean()`
```{python}
mpg.mean()
```


```{python}
20.090625
```


```{python}
mpg.sum()
```

```{python}
642.9
```

```{python}
mpg.stdev()
```

```{python}
5.9320295523012181
```

`variance()`

```{python}
mpg.variance()
```

```{python}
35.188974609374995
```


`stats()`

```{python}
mpg.stats()
```

```{python}
(count: 32, mean: 20.090625, stdev: 5.9320295523, max: 33.9, min: 10.4)
```


## Group functions

* `groupBy()`
* `groupByKey()`
* `cogroup()`
* `groupWith()`

### The `groupBy()` function
this function applies a function to each RDD's elements and then use the returned values to group the elements. The `groupBy()` returns a key-value pair RDD. The keys are returned values from the applied function, the values are **interable sequences**. To show the real elements, you can convert these sequences to lists.

```{python}
rdd = sc.parallelize([1,2,3,4,5, 'a','b','c'])
rdd.collect()
```

```{python}
[1, 2, 3, 4, 5, 'a', 'b', 'c']
```

Apply the `groupBy()` function.

```{python}
rdd.groupBy(lambda x: isinstance(x, str)).collect()
```

```{python}
[(False, <pyspark.resultiterable.ResultIterable at 0x10c5cde50>),
 (True, <pyspark.resultiterable.ResultIterable at 0x10c5fd0d0>)]
```

Convert values in the key-value pair RDD to lists.

```{python}
rdd.groupBy(lambda x: isinstance(x, str)).mapValues(list).collect()
```

```{python}
[(False, [1, 2, 3, 4, 5]), (True, ['a', 'b', 'c'])]
```


### The `groupByKey()` function

This function is similar to `groupBy()`. The `groupBy()` needs a function to generate keys for the RDD's elements. The `groupByKey()` function operate directly RDD that already has keys.

```{python}
rdd = sc.parallelize([
        ('a', [1,2]),
        ('b', [3,4]),
        ('a', [1,5]),
        ('b', [2,3])
    ])
 
rdd.groupByKey().mapValues(list).collect()    
```

```{python}
[('a', [[1, 2], [1, 5]]), ('b', [[3, 4], [2, 3]])]
```

### The `cogroup()` function

Merge two key-value pair RDDs by keys. The values in the returned RDD is a tuple. The first element in the tuple is the value from the first RDD, the second value inthe tuple is the value from the second RDD.

```{python}
x_rdd = sc.parallelize([
        ('a', [1,2]),
        ('b', [2,4]),
        ('a', [5])
    ])
y_rdd = sc.parallelize([
        ('a', [8]),
        ('c', [10])
    ])
```

```{python}
x_rdd.cogroup(y_rdd).collect()
```

```{python}
[('a',
  (<pyspark.resultiterable.ResultIterable at 0x10c6753d0>,
   <pyspark.resultiterable.ResultIterable at 0x10c543b10>)),
 ('c',
  (<pyspark.resultiterable.ResultIterable at 0x10c5432d0>,
   <pyspark.resultiterable.ResultIterable at 0x10c543710>)),
 ('b',
  (<pyspark.resultiterable.ResultIterable at 0x10c543fd0>,
   <pyspark.resultiterable.ResultIterable at 0x10c543290>))]
```

```{python}
x_rdd.cogroup(y_rdd).mapValues(lambda x: [map(list, i) for i in x] ).collect()
```

```{python}
[('a', [[[1, 2], [5]], [[8]]]), ('c', [[], [[10]]]), ('b', [[[2, 4]], []])]
```

## Merge functions

These functions merge values from two RDDs and generate one RDD:

* `union()`
* `zip()`
* `zipWithIndex()`

The `union()` function append one RDD to the other.

```{python}
x = sc.parallelize([1,2,3])
y = sc.parallelize([4,5,6])
x.union(y).collect()
```

```{python}
[1, 2, 3, 4, 5, 6]
```


The `zip()` function combines two RDDs and return a key-value pair RDD. The elements from the first RDD become the keys, and the elements from the second RDD become the values.

```{python}
x = sc.parallelize(['a', 'b', 'c'])
y = sc.parallelize([1, 2, 3])
x.zip(y).collect()
```

```{python}
[('a', 1), ('b', 2), ('c', 3)]
```

The `zipWithIndex()` function combines an RDD's elements with element indices and returns an RDD. 

```{python}
x = sc.parallelize(['a', 'b', 'c'])
x.zipWithIndex().collect()
```

```{python}
[('a', 0), ('b', 1), ('c', 2)]
```






